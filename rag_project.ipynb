{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1b6827ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer, util, CrossEncoder\n",
    "import openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "49a6291d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 1. API key for OpenAI GPT\n",
    "# ---------------------------\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1f9c6dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 2. Load dataset\n",
    "# ---------------------------\n",
    "df = pd.read_csv(\"IR_dataset_entries.csv\")\n",
    "corpus = df['content'].tolist()\n",
    "\n",
    "# ---------------------------\n",
    "# Load Evaluation Dataset\n",
    "# ---------------------------\n",
    "eval_df = pd.read_csv(\"evaluation_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f7afab94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 3. BM25 Sparse Retrieval\n",
    "# ---------------------------\n",
    "tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "45d8f57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 4. Dense Retrieval\n",
    "# ---------------------------\n",
    "dense_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "corpus_embeddings = dense_model.encode(corpus, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c0f63cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 5. Cross-Encoder for Re-ranking\n",
    "# ---------------------------\n",
    "cross_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7f240623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Relevant IDs</th>\n",
       "      <th>Retrieved IDs</th>\n",
       "      <th>Precision@5</th>\n",
       "      <th>Recall@5</th>\n",
       "      <th>MRR</th>\n",
       "      <th>nDCG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is BM25?</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[999, 412, 997, 820, 800]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is semantic search?</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[380, 369, 378, 464, 36]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Explain embedding techniques.</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[1, 17, 18, 451, 303]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is information retrieval?</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[914, 673, 838, 585, 724]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is query understanding?</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[972, 837, 732, 995, 174]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Query Relevant IDs              Retrieved IDs  \\\n",
       "0                   What is BM25?          [4]  [999, 412, 997, 820, 800]   \n",
       "1        What is semantic search?          [5]   [380, 369, 378, 464, 36]   \n",
       "2   Explain embedding techniques.          [1]      [1, 17, 18, 451, 303]   \n",
       "3  What is information retrieval?          [3]  [914, 673, 838, 585, 724]   \n",
       "4    What is query understanding?          [2]  [972, 837, 732, 995, 174]   \n",
       "\n",
       "   Precision@5  Recall@5  MRR  nDCG  \n",
       "0          0.0       0.0  0.0   0.0  \n",
       "1          0.0       0.0  0.0   0.0  \n",
       "2          0.2       1.0  1.0   1.0  \n",
       "3          0.0       0.0  0.0   0.0  \n",
       "4          0.0       0.0  0.0   0.0  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load evaluation dataset\n",
    "eval_df = pd.read_csv(\"evaluation_set.csv\")\n",
    "\n",
    "# ---------------------------\n",
    "# Evaluation metric functions\n",
    "# ---------------------------\n",
    "def compute_precision(relevant_docs, retrieved_docs):\n",
    "    return len(set(relevant_docs).intersection(set(retrieved_docs))) / len(retrieved_docs)\n",
    "\n",
    "def compute_recall(relevant_docs, retrieved_docs):\n",
    "    return len(set(relevant_docs).intersection(set(retrieved_docs))) / len(relevant_docs)\n",
    "\n",
    "def compute_mrr(relevant_docs, ranked_docs):\n",
    "    for rank, doc in enumerate(ranked_docs, start=1):\n",
    "        if doc in relevant_docs:\n",
    "            return 1 / rank\n",
    "    return 0\n",
    "\n",
    "def compute_ndcg(relevant_docs, ranked_docs):\n",
    "    def dcg(scores):\n",
    "        return sum(score / np.log2(i + 2) for i, score in enumerate(scores))\n",
    "    relevance_scores = [1 if doc in relevant_docs else 0 for doc in ranked_docs]\n",
    "    ideal_scores = sorted(relevance_scores, reverse=True)\n",
    "    if sum(ideal_scores) == 0:\n",
    "        return 0\n",
    "    return dcg(relevance_scores) / dcg(ideal_scores)\n",
    "\n",
    "# ---------------------------\n",
    "# Run evaluation\n",
    "# ---------------------------\n",
    "results = []\n",
    "\n",
    "for i, row in eval_df.iterrows():\n",
    "    query = row[\"query\"]\n",
    "    relevant_ids = eval(row[\"relevant_ids\"])  # \"[4]\" → [4]\n",
    "\n",
    "    # Run your RAG pipeline\n",
    "    answer, retrieved_docs = chat_with_rag(query)\n",
    "\n",
    "    # Convert retrieved docs → doc IDs\n",
    "    retrieved_ids = []\n",
    "    for doc in retrieved_docs:\n",
    "        doc_id = df[df[\"content\"] == doc][\"id\"].values[0]\n",
    "        retrieved_ids.append(doc_id)\n",
    "\n",
    "    # Compute metrics\n",
    "    precision = compute_precision(relevant_ids, retrieved_ids)\n",
    "    recall = compute_recall(relevant_ids, retrieved_ids)\n",
    "    mrr = compute_mrr(relevant_ids, retrieved_ids)\n",
    "    ndcg = compute_ndcg(relevant_ids, retrieved_ids)\n",
    "\n",
    "    results.append({\n",
    "        \"Query\": query,\n",
    "        \"Relevant IDs\": relevant_ids,\n",
    "        \"Retrieved IDs\": retrieved_ids,\n",
    "        \"Precision@5\": precision,\n",
    "        \"Recall@5\": recall,\n",
    "        \"MRR\": mrr,\n",
    "        \"nDCG\": ndcg\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ee8a5df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 6. Chatbot Loop\n",
    "# ---------------------------\n",
    "def chat_with_rag(query, top_k=5):\n",
    "    # ----- BM25 initial retrieval -----\n",
    "    tokenized_query = query.split(\" \")\n",
    "    bm25_top = bm25.get_top_n(tokenized_query, corpus, n=top_k*2)  # retrieve more for cross-ranking\n",
    "\n",
    "    # ----- Dense retrieval -----\n",
    "    query_embedding = dense_model.encode(query, convert_to_tensor=True)\n",
    "    dense_hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=top_k*2)\n",
    "    dense_top = [corpus[h['corpus_id']] for h in dense_hits[0]]\n",
    "\n",
    "    # Combine BM25 + Dense results\n",
    "    combined_top = list(dict.fromkeys(bm25_top + dense_top))[:top_k*3]\n",
    "\n",
    "    # ----- Cross-Encoder Re-ranking -----\n",
    "    cross_scores = cross_model.predict([[query, doc] for doc in combined_top])\n",
    "    top_docs = [doc for _, doc in sorted(zip(cross_scores, combined_top), reverse=True)][:top_k]\n",
    "\n",
    "    # ----- Contextual Answer Generation -----\n",
    "    context = \"\\n\".join(top_docs)\n",
    "    prompt = f\"Answer the question based on the context below:\\n\\nContext: {context}\\n\\nQuestion: {query}\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    max_tokens=300\n",
    ")\n",
    "\n",
    "    answer = response.choices[0].message.content\n",
    "    return answer, top_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dfe25262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Chatbot Ready! Type 'exit' to quit.\n",
      "\n",
      "(No evaluation available for this query)\n",
      "\n",
      "Bot Answer:\n",
      " Machine learning is a subset of artificial intelligence that involves the development of algorithms and statistical models that enable computers to perform tasks without explicit instructions. Instead of relying on hard-coded rules, machine learning systems learn from data to identify patterns and make decisions. This process typically involves training a model on a dataset, allowing it to adapt and improve over time. Machine learning can be applied to various fields, including information retrieval, where it helps in organizing, ranking, and retrieving information efficiently based on user queries.\n",
      "\n",
      "Top Retrieved Contexts:\n",
      "1. Information Retrieval Basics content example 710. This explains key concepts of LLM in IR.\n",
      "2. Information Retrieval Basics content example 704. This explains key concepts of LLM in IR.\n",
      "3. Information Retrieval Basics content example 427. This explains key concepts of LLM in IR.\n",
      "4. Information Retrieval Basics content example 658. This explains key concepts of LLM in IR.\n",
      "5. Information Retrieval Basics content example 691. This explains key concepts of LLM in IR.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 7. User Interaction\n",
    "# ---------------------------\n",
    "print(\"RAG Chatbot Ready! Type 'exit' to quit.\")\n",
    "while True:\n",
    "    user_query = input(\"You: \")\n",
    "    if user_query.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # Run RAG pipeline\n",
    "    answer, retrieved_docs = chat_with_rag(user_query)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Evaluation Section\n",
    "    # ---------------------------\n",
    "    # Convert retrieved docs back to their document IDs\n",
    "    retrieved_ids = []\n",
    "    for doc in retrieved_docs:\n",
    "        doc_id = df[df[\"content\"] == doc][\"id\"].values[0]\n",
    "        retrieved_ids.append(doc_id)\n",
    "\n",
    "    # Check if this query exists in evaluation_set.csv\n",
    "    matching_eval = eval_df[eval_df[\"query\"] == user_query]\n",
    "\n",
    "    if len(matching_eval) > 0:\n",
    "        # Extract relevant IDs\n",
    "        relevant_ids = matching_eval[\"relevant_ids\"].values[0]\n",
    "        relevant_ids = eval(relevant_ids)  # Convert string \"[4]\" → list [4]\n",
    "\n",
    "        # Compute metrics\n",
    "        precision = compute_precision(relevant_ids, retrieved_ids)\n",
    "        recall = compute_recall(relevant_ids, retrieved_ids)\n",
    "        mrr = compute_mrr(relevant_ids, retrieved_ids)\n",
    "        ndcg = compute_ndcg(relevant_ids, retrieved_ids)\n",
    "\n",
    "        print(\"\\nEvaluation Metrics:\")\n",
    "        print(\"Precision:\", precision)\n",
    "        print(\"Recall:\", recall)\n",
    "        print(\"MRR:\", mrr)\n",
    "        print(\"nDCG:\", ndcg)\n",
    "    else:\n",
    "        print(\"\\n(No evaluation available for this query)\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # Chatbot Results\n",
    "    # ---------------------------\n",
    "    print(\"\\nBot Answer:\\n\", answer)\n",
    "\n",
    "    print(\"\\nTop Retrieved Contexts:\")\n",
    "    for idx, doc in enumerate(retrieved_docs, 1):\n",
    "        print(f\"{idx}. {doc}\")\n",
    "\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
